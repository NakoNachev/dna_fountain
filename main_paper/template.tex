\documentclass[12pt]%[english]
{article}

\usepackage{graphicx}
\usepackage{microtype}
\usepackage[nonumberlist, acronym, toc, section]{glossaries}
\usepackage{cite} 
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{caption}


\newglossary[slg]{symbolslist}{syi}{syg}{Symbols}

\makeglossaries

%commands for symbols
\newglossaryentry{symb:Pi}{
name=$\pi$,
description={You know it.},
sort=symbolpi, type=symbolslist
}
\newglossaryentry{symb:Phi}{
name=$\varphi$,
description={At vero eos et accusam et justo duo dolores et ea rebum..},
sort=symbolphi, type=symbolslist
}
\newglossaryentry{symb:Lambda}{
name=$\lambda$,
description={Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy.},
sort=symbollambda, type=symbolslist
}
 
%commands for abbreviations
\newacronym{MS}{MS}{Microsoft}
\newacronym{CD}{CD}{Compact Disc}

% ...

%abbreviations and glossary combined
\newacronym{AD}{AD}{Active Directory\protect\glsadd{glos:AD}}
 
% ... 
 
%glossary commands

\newglossaryentry{glos:AD}{
name=Active Directory,
description={At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet}
}

\newglossaryentry{glos:F}{name=File, description={An arbitrary file}}

% ...

\begin{document}

\begin{titlepage}

\begin{center}

{\Huge %\bfseries
 
{%\textbf{
\textsc{DNA Fountain encoding}}}
%}
\\[2ex]




\textbf{
\Large Seminar Datenmanagement\\ 
Prof. Dr. Lena Wiese \\ 
Wintersemester 23/24
}

\includegraphics[scale=0.4]{logo.jpg} \\ 
\large{\textbf{Institute of Computer Science \\ Goethe-Universit\"at Frankfurt a. M.}}



\begin{Large}
\begin{tabular}{ll}
\\
Author: & \textsc{Nako Nachev} \\
& 6464679\\
& s6327063@stud.uni-frankfurt.de \\
Degree:&  Master Informatik, Semester 7 \\
Module ID: & DM-MS, M-DS-S, DM-BS, B-ATAI-S \\
Date: & \today \\		
\end{tabular}
\end{Large}

\end{center}

\vspace*{\fill}

\large
\noindent{}Work based on: \\
\begin{itemize}
\item Paper 1: \emph{Mercier, Hugues and Bhargava, Vijay K and Tarokh, Vahid}: A survey of error-correcting codes for channels with symbol synchronization errors
\item Paper 2: \emph{Erlich, Yaniv and Zielinski, Dina} DNA Fountain enables a robust and efficient storage architecture
\end{itemize}


\end{titlepage}

\newpage\thispagestyle{empty}~ %empty page
\newpage 

\begin{abstract}
This article provides a brief overview of different error-correcting codes and their specifics, such as usage and capability of correcting errors of various kinds. Subsequently, it describes a specific storage strategy called DNA Fountain by examining the different stages, such as encoding and decoding, and provides information about the exact construction and experiments specifics. Furthermore, a python implementation of the storage strategy is suggested, serving as a testing ground to experiment with this storage mechanism. Moreover, two digital logos of Fraunhofer institute are supplied as a benchmark dataset and are used as input data for the python scripts. Lastly, the article concludes with criticism on the topic and lists potential problems or future improvements for the storage strategy.
\end{abstract}

\newpage

\tableofcontents

\newpage

\listoffigures
\listoftables

\newpage

\section{Introduction}

\subsection{Summary Overview Paper}

The paper in \cite{mercier2010survey} provides a detailed survey about different error correcting codes, their implementation
and associated obstacles with each of the codes. The paper begins by examining the historical significance of error synchronization problems in
communication systems dating back to Samuel Morse, which remain an important topic even in today's technological systems. Although often treated as
different problems, synchronisation and additive noise tend to have the same effect on communication channels by reducing their capacity.According to the 
article, designing error-correcting codes to tackle these problems proves to be quite challenging, with many techniques of implementation not based on coding or even 
failing to mention synchronisation-correcting codes. \\
This article continues with a brief introduction on the error-correcting codes and explaining the 
different preliminaries associated with the survey. Such preliminaries include the mathematical notations used, the different types of synchronisation errors and their definitions.
These range from deletion errors, insertion errors and duplication or repetition errors to bitshift errors. Last but not least, obstacles associated with the 
error correcting codes are described. Examples of such are huge bursts of substitution errors, splitting long messages of data into blocks with boundaries
sometimes unknown to the receiver or codes being unable to keep the same correction capability when encoding several blocks. The authors describe in section III different 
synchronisation-correcting codes, how they are implemented, the historic research done by various authors and specific limitations for the different types. The section starts
with binary algebraic block codes using (0,1) alphabets, nonbinary and perfect codes, codes capable of correcting bursts of synchronisation errors, synchronizable, marker codes, codes for weak
synchronisation errors, convolutional codes, expurgated and codes for random synchronisation channels. \\
The last section of the paper discusses future directions of synchronisation error-correcting codes. Binary block codes are currently unable to detect more than one synchronisation
errors per block. Further challenges arise in estimating the capacity of channels corrupted by synchronisation errors as well as situations where random deletion and/or insertion errors can occur.
According to the authors, none of the codes described in the paper has all the properties required to be used in pratical communication systems. One solution could be concatenating different
coding schemes in order to use the strengths of the various codes for their specific implementation use cases. \\
Drawing from the examples in the preciding article, one could divide the error-correcting codes in multiple categories. One separation used could be based on the number of symbols used in the alphabet - either binary alphabet consisting of (0,1)* symbols or non-binary with 2 or more symbols. Another categorization could follow based on other characteristics - such as block codes vs convolutional codes. Block codes break the data down to fixed-sized blocks and append protective or unique bits for error handling. Convolutional codes on the other side process the data continuously as a stream and do not segment the data into blocks. Error correction here is done on the fly. \\
As far as characteristics of the error-correcting codes are concerned, performance, redundancy, error detection and correction seem to be the most important ones. Redundancy finds its application in appending the extra bits for the receiver to make use of and identify errors that may have happened during the transmition. Performance concerns the rate at which algorithms are able to correct the data, where similar to most systems a good balance between correctness achived and computation should be achieved. Error detection and correction seems to be closely related and are the general characteristics that prove how valuable a given error-correcting code is. \\
There are various implementations of error-correcting codes. One such example is by using error-correcting output coding for improving text classification and thus reducing text classification errors by 66 percent.\cite{ghani2000using} Another example is by using error-correcting codes for real time communication in wireless networks. \cite{elaoud1998adaptive} The authors simulate using an adaptive scheme for reducing bandwidth overhead by comparing it to a single code scheme.


\subsection{Important Terms and Definitions}

In the context of error-correcting codes for channels with synchronisation errors, following important terms and definitions can be listed based on the information provided in \cite{mercier2010survey}:
\begin{itemize}
\item \textbf{Synchronisation error}: These are either a deletion or an insertion error and excludes substitution errors. For example transmitting 0011 instead of 00011 is a deletion error and transmitting 0011 instead of 011 - insertion error.
\item \textbf{Duplication or repetition error}: These errors are a special insertion that replaces a bit by two copies of the same bit.
\item \textbf{Bitshift error}: A special kind of error that might reverse a given string - for example 01 being transmitted as 10.
\item \textbf{Additive noise}: Disturbances or interference when transmitting signal over a communication network
\item \textbf{Redundancy}: Additional information introduced by error-correcting codes to correction synchronisation errors 
\item \textbf{Run}: A substring of identical symbols of maximum length
\item \textbf{Block}: Used to form chunks of words that are used to divide long messages
\end{itemize}

The concepts and terms provided in \cite{mercier2010survey} are crucial for understanding the challenges in designing error-correcting codes for channels with erasures. Due to the specifics and various mechanisms for correcting errors, one such method is described in  \ref{sec:in_depth} to give more clarity and details about the implementation.

\newpage

\section{In-Depth Explanation of Error-correcting Code Chosen}\label{sec:in_depth}

Current storage medias do not have the capacity to meet the demand for the total amount of worldwide data. Moreover, the costs for maintaining and transferring data are increasing, therefore better solutions are needed \cite{dong2020dna}. One such promising solution is the innovative DNA storage systems. These systems allow us to store digital data into DNA molecules, which are synthesized and far exceed the lifespan of general storage mediums \cite{dong2020dna}. However, transferring and storing data comes with their complications and certain errors may occur, leading to corrupted or incomplete data. Thus, solutions for correcting these errors are necessary. The storing strategy described in \cite{erlich2017dna} makes use of the so-called DNA fountains. Fountain codes are typically used in channels with erasures, where files are split into packets and each packet is either received without errors or dropped \cite{mackay2005fountain}.  Standard transfer protocols keep transmitting packets until they are successfully received from the other side, where another channel is also required to keep track of the state of transmitted packets or those that require retransmission. Fountain codes on the other side create packets as a result of functions of the whole file and the receiver only needs to collect any N packets, with $N \textgreater K$ (original size), to recover the whole file.In the metaphorical sense, a dna fountain generates droplets representing packets of data and anyone wishing to collect these droplets has to place a bucket under the fountain and wait until the amount collected exceeds the original size K of the data \cite{mackay2005fountain}. Due to the nature of the fountain,  DNA fountains can be classified as rateless, since the number of packets generated can be limitless.
The strategy for data storage using DNA fountains proposed in \cite{erlich2017dna} works in two stages.

\subsection{DNA Fountain encoding}

The first stage is the encoding phase, which consists of three stages - preprocessing, luby transform and screening.

\subsubsection{Preprocessing}  
The first step of the encoding is the preprocessing stage. It begins by packaging the files we want to encode in a tar file and compressing them using stardard lossless algorithms \cite{erlich2017dnasupplementary}.It partitions the file into non-overlapping segmets of identical length. A simple example would be splitting the code 001001100101 into segments of length 4, resulting in the following segments:
\begin{itemize}
\item Segment 1: 0010
\item Segment 2: 0110
\item Segment 3: 0101
\end{itemize}

\subsubsection{Luby Transform}
The next step of the encoding is the so-called \textbf{Luby-transform stage}. The first job of the Luby Transform stage is to initialize a pseudorandom number generator (PRNG) with the help of a seed. The seed used by the authors in \cite{erlich2017dnasupplementary} corresponds to the current state of the PRNG, with each cycle for the seed generated with the help of a Linear Feedback Shift Register (LFSR) \cite{erlich2017dnasupplementary}. An LFSR is a shift register whose input bit is a linear function of its previous state \cite{hathwalia2014design}. A LFSR uses a polynomial to generate the so called "taps", which correspond to the coefficients of the polynomial. It works by first shifting the n-bit large string to the right and then XORs together the bits from the previous state corresponding to the "tap" positions. Given the right polynomial, a LFSR can be considered a "maximum-length LFSR", since it is able to produce \(2^{n}-1\) states before returning to the initial state \cite{hathwalia2014design}. In the context of the dna fountain strategy, such LFSR will examine each seed in a given interval without repetition \cite{erlich2017dnasupplementary}. Below is a quick explanation on how the LFSR works: 

\begin{itemize}
\item Suppose we generate a 3-bit seed
\item Set the initial seed: 100
\item Provide a polynomial for the LFSR: \(x^{2} + x + 1\)
\item The above polynomial defines taps on positions (2,1)
\item Shift the seed to the right:  \texttt{\_10}
\item XOR together the bits under the tap positions: \(0 \oplus 0 = 0\)
\item Append the result from the XOR operation to the left, leaving us with \textbf{010} as next state 
\end{itemize}

After generating a seed, the next job of the Luby Transform is to decide how many of the segments to combine together. This is achieved with the help of a special robust soliton distribution, defined as follows \cite{erlich2017dnasupplementary}:   

\[
\rho(d) = 
\begin{cases} 
\frac{1}{K} & \text{if } d = 1, \\
\frac{1}{d(d-1)} & \text{for } d = 2, \dots, K.
\end{cases}
\]

\[
\beta(d) = 
\begin{cases} 
\frac{s}{Kd} & \text{for } d = 1, \dots, \left(\frac{K}{s}\right) - 1, \\
\frac{s \ln(s/\delta)}{K} & \text{for } d = \frac{K}{s}, \\
0 & \text{for } d > \frac{K}{s},
\end{cases}
\]


\[
\mu_{K,c,\delta}(d) = \frac{\rho(d) + \beta(d)}{Z},
\]

with 

\[
s = c \sqrt{\frac{K}{\ln^2\left(\frac{K}{\delta}\right)}} \]

and 
\[
Z = \sum_{d} \rho(d) + \beta(d) = 1
\]


After the distributions for the robust soliton distribution are calculated, N segments are sampled at randomly without replacement (based on the choice from soliton) and  than added bitwise together. The seed generated from the LFSR is attached to the result of the bitwise addition, thus forming the final droplet. One simple example might be:


\begin{itemize}
\item Choose two random segments: 0010 and 0110
\item Apply bitwise addition: \( 0010 \oplus 0110 = 0100 \)
\item Generate seed: 11
\item Attach seed to the result of the bitwise addition to form the droplet: 110100
\end{itemize}


Apart from the seed and payload, extra redundancy can be added to the droplet in order for it to be able to correct various errors that may appear during the syntesizing process. This is where Reed-Solomon codes (R-S) come into place. Reed-Solomon codes are nonbinary cyclic codes with symbols up to m-bit, with m being any positive integer having a value greater than 2. R-S (n,k) codes of m-bit symbols comply to the following rules:
\begin{center}

$0 \textless k \textless n \textless 2^m +2$

\end{center}

with k being the number of symbols encoded and n the total number of symbols in the encoded block \cite{sklar2001reed}. Furthermore, R-S(n,k) codes also include parity symbols:

\begin{center}

$(n,k)=(2^m - 1, 2^m -1 - 2t)$

\end{center}

with $2t = n - k$ being the number of parity symbols \cite{sklar2001reed}. For example a R-S(5,3) 3-bit code  contains 5 code word bytes, of which 3 are data and 2 parity (n-k).Therefore, this code is capable of correcting up to $2t=2   => t = 1$ errors.


\subsubsection{Screening}
The Luby transform stage concludes with a screening stage.This stage first converts the achieved droplet to DNA using the following conversion rules: \{00,01,10,11\} to \{A,C,G,T\} \cite{erlich2017dnasupplementary}. After that, the result dna sequence is screened for invalid sequences, such a homopolymer runs or GC content. Long homopolymer runs and sequences with high GC concentration are undesirable \cite{erlich2017dna} due to being more difficult to syntesize or are more prone to sequencing errors. DNA sequences containing one of the above described properties are rejected and those that pass added to the oligo design file. In \cite{erlich2017dna}, the created droplets were 38 bytes (4 bytes for seed, 32 bytes for data payload and 2 bytes for RS code). Therefore, the values for homopolymer runs were set to \(<= 3 nt \) (nucleotides) and 45-55\% GC content, but these values could be changed based on the specific case. Below is the screening stage described with continuation of the example above:

\begin{itemize}
\item Convert the droplet to a DNA sequence: $110101 \rightarrow TCC$
\item Check against screening rules: TCC does not contain long homopolymer runs or high \% GC content, therefore does not get rejected
\item The oligo file now contains the droplet TCC
\end{itemize}

Since quite a few of the oligos can fail, the authors in \cite{erlich2017dna} recommend aiming for 5-10\% more oligos than input segments. The process of luby transform and screening stage is repeated until that number is reached. 

\newpage
\subsection{DNA Fountain decoding}

The decoding part of the DNA Fountain encoding strategy consists also of 3 stages - preprocessing, droplet recovery and segment interference.

\subsubsection{Preprocessing}

The preprocessing phase of the decoding process begins by filtering out all sequences that do not equal the predefined length \cite{erlich2017dnasupplementary}. Subsequently, idential sequences are dropped and the number of occurences is stored, ensuring that more prevalent sequences will appear first when sorted. As the decoder may not always need to scan all oligos, some of the singletons (sequences that have been observed only a small number of times) may be omitted and the decoder would not try to decode them. Below is a simple example illustrating the process:

\begin{itemize}
\item Assume these are the generated oligos from the encoding process: GAT, TAG, CAG, GTC, GAT, AAG, GAT, GGAT, CAG
\item Step 1: Remove oligos larger than 3nt - GGAT is filtered out
\item Step 2: Calculate occurrences - GAT: 3, TAG: 1, CAG: 2, GTC: 1, AAG: 1
\item Step 3: Sort sequences based on occurrence - GAT:3, CAG: 2, TAG: 1, GTC: 1, AAG: 1
\end{itemize}

\subsubsection{Droplet Recovery}\label{subsec:droplet_recovery}

The second phase of the decoding process is droplet recovery. The generated DNA sequences from the encoding process have to first be translated back into binary using the following scheme, as detailed in  \cite{erlich2017dnasupplementary}: {A,C,G,T}, to {0; 1; 2; 3}. In addition to translating the sequences, the droplet recovery stage must also extract the seed, payload and error-correcting code (if it exists). Unlike other methods, the in \cite{erlich2017dna} described approach does not try to recover sequences with errors. Instead, they are excluded without attempting to correct them. Most of the errors found are mainly due to short insertions and deletions that may result during the oligo synthesis. The Reed Solomon code used in the encoding process is designed to handle only substitution errors, therefore it is not advisable to attempt recovery, as mentioned in \cite{erlich2017dnasupplementary}.


\subsubsection{Segment inference}

The final step in the decoding process is the segment inference. Once the integrity of the message is verified (meaning that all checks from the previous stages have passed), the decoder initializes the PRNG with the extracted seed obtained from \ref{subsec:droplet_recovery}. Using this seed, a list of segment identifiers can be generated as described in \cite{erlich2017dnasupplementary}. These identifiers represent potential positions of the input segments that were combined to form the droplet, as the same seed was used for the PRNG initialization during the encoding process. \\
Next, a message-passing algorithm is initiated to try and infer the segments of each droplet. The algorithm first checks whether the droplet contains segments that have been previously inferred. If such are found, they are XOR-ed from the droplet, and their index is removed from the list of indices. When only one segment remains in the droplet, the segment will be assigned to the droplet data payload. The information regarding the inferred segments is propagated to the remaining droplets, and the entire algorithm is recursively repeated until no more updates can be made. This leads to the number of solved segments slowly increasing until the number of observed oligos reaches the number of segments. At this point, the number of solved segments explodes, as illustrated in the picture below:

 \begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{decoding_oligos}
    \caption{Solved to observed oligos comparison \cite{erlich2017dnasupplementary}}
\end{figure}


\section{Implementation of  Error-correcting Code Chosen}

\subsection{DNA Fountain implementation}\label{subsec:implementation}

The suggested in \cite{erlich2017dna} dna fountain method was implemented in python by making use of standard libraries and a couple of extra libraries for ready implementations of the LFSR and RS-Codes. The project contains the following files:
\begin{itemize}
\item \texttt{dna\_fountain.py}
\item decoder.py
\item helper.py
\item \texttt{image\_creator.py}
\item models.py
\item soliton.py
\item input.py
\end{itemize}

The starting point of the project is the \texttt{dna\_fountain.py} file. The file starts by initializing the parameters required for the soliton distribution and the encoding process. The first step involves choosing the input data to use. For the purpose of the seminar, two different inputs were predefined, (\texttt{picture\_short} and \texttt{picture\_long}), both of which are saved in the input.py file. Both inputs are Python lists with 0s and 1s, corresponding to the different bits saved from the pictures. Based on the chosen input, different globals and defaults are created. Examples of such are \texttt{segment\_size}, \texttt{initial\_seed}, polynomial and \texttt{segments\_multiplier}. The function continues by first splitting the input list into multiple segments based on the initialized parameters. If the input file is not divisible by the number of bits per segment because of the chosen segment size, the last segment is filled with 0s on the left side to align its size with the other segments. After the list of segments is created, it is passed to the \texttt{robust\_soliton\_distribution} function from the soliton.py file, which returns the distribution of the values based on the total number of segments generated. Soliton.py also contains a \texttt{ideal\_soliton\_distribution} function, which is not used, but remains an option. \\
Once all defaults, parameters, and values are initialized, the oligo creation process begins with the \texttt{oligo\_creation} functions. It first defines the total number of oligos to be generated and iterates until the desired number is reached. The \texttt{luby\_transform} function is responsible for the creation of oligos. It first creates a droplet (with the correspondig seed, data payload and RS-Code (if exists)) and checks it against homopolymer runs or high GC content. If any of the conditions are met, the function will not return anything, meaning the iterator has to try again. Otherwise, the function returns the created oligo, which is then passed to the final list. The core of the \texttt{luby\_transform} function is \texttt{create\_droplet\_bin}. It creates the seed using the LFSR with the predefined parameters (seed, polynomial), and chooses a number N of segments to combine, assisted by the generated distribution from the robust soliton implementation.  The seed is then used to generate N indices for random segments of the list, which are XOR-ed together to form the final payload of the droplet. Along the way, different helper functions and models are used. Examples of helper functions include binary arrays to bytes, XOR-ing integers or strings or different utility functions, all of which provided in helper.py file. Models.py contains classes representing different types, which, while not necessary in python due to the absence of static checking, may improve readability and debugging (if necessary). \\
After the oligos are created, they, along with other parameters, are passed to the decoder.py file. This file starts by preprocessing the oligos. The first two stages of preprocessing, as suggested by \cite{erlich2017dnasupplementary}, involve retaining only sequences of a certain length and collapsing identical sequences and sorting them so that sequences that occur more often are at the front of the list. Since the created oligos are still in string format (due to the use of nucleotides represented by the letters A,C,G,T), we need to convert the data back to binary. This is achieved via the \texttt{create\_oligo} function, which also extracts the seed, data payload and potential error correcting codes that were appended at the end of the oligo. As described in \cite{erlich2017dna}, the process does not attempt to make error corrections and for that purpose oligos with errors are dropped and do not fall into the decoding process. Last but not least, the \texttt{recursively\_infer\_segments} function is called, which is the final stage of the decoding process. It iterates through the preprocessed oligos, and attempts to infer the segments until no more updates can be made. Finally, it returns the inferred segments and they are passed back to the \texttt{dna\_fountain.py} file. \\
The final part of the \texttt{dna\_fountain.py} file is the image creation. The \texttt{image\_creator.py} file is called, which contains a function for generating a .PNG picture based on the returned data (list of 0s and 1s). That way we can compare the output picture to the initial picture and compare whether we have successfully decoded the initial file. Although some differences in the bits may occur, which most of the time might not be noticable with a human eye, the \texttt{calc\_similarity} function from the helper.py file can compare both input and output lists and return a value between [0,1] indicating how similar the decoded file is to the initial. Value of 1 means the initial file was decoded without any errors or missing information, anything below it would indicate missing or incorrect data.


\subsection{Benchmark Results}\label{subsec:benchmark_results}

In order to test the decoding power and correctness of the in 3.1 described implementation, a benchmark dataset comprised of two different datasets was used. The first dataset is the leftmost logo part of the Fraunhofer Institute logo \ref{fig:logo_short}, which is afterwards converted to a python list of 0s and 1s and saved into the input.py file. The second dataset in \ref{fig:logo_long} is the complete Fraunhofer logo, which is then converted to a binary list similarly to the transformation of the first dataset. Both of the generated pictures are black and white and do not take into account the original colours of the logo. For testing purposes a third set was also used, comprised of only 16 bits, which enables easier tracking of the whole encoding/decoding process.  \\
White creating output pictures based on the decoded data a 100\% completeness was achieved for both logos. Depending on the chosen logo, different parameters were used for the various stages in the encoding/decoding. Examples of such parameters are the size of the segments in bits, the initial seed, the chosen polynomial for the LFSR and the segments multiplier. Both c and delta values for the soliton distribution remained the same, although they could just as well be made to change accordingly, based on predefined conditions. The table below describes the parameters used for each of the tests: \\


\begin{table}[h!]
    \centering
    \small
    \begin{tabularx}{\textwidth}{|X|X|X|X|}
        \hline
        Dataset & Minimal dataset & Short dataset & Long dataset \\
        \hline
        Segment size & 4 & 32 & 4 \\
        \hline
        Initial seed & 1000 & 1..0 & 1..0 \\
        \hline
        Seed size & 4 bits & 32 bits & 32 bits \\
        \hline
        Polynomial & \(x^{4} + x^{3} + 1 \) & \(x^{32} + x^{30} + x^{26} + x^{25} + 1\) & \(x^{32} + x^{30} + x^{26} + x^{25} + 1\) \\
        \hline
        Segments multiplier & 5 & 3.5 & 3 \\
        \hline
        Soliton c value & 0.2 & 0.2 & 0.2 \\
        \hline
        Soliton delta value & 0.001 & 0.001 & 0.001 \\
        \hline
    \end{tabularx}
    \caption{Benchmark test parameters}
    \label{tab:benchmark-parameters}
\end{table}

Several noteworthy aspects consider the chosen parameters for the different inputs. One such aspect is the segments multiplier. When testing with the minimal input (16 bits), a higher amount of oligos needs to be generated in comparison to the input segments. The main reason for that is since we are picking segments at random, there exists the possibility of not picking a certain segment, which would lead to not being able to fully restore the file. Another important consideration is the seed size. Since we are preprocessing the input into non-overlapping segments, choosing a segment size which does not split the input into equal size can introduce difficulties. For example filling the remaining segments with leading 0 bits may be necessary to match the required size. Furthermore, the initial seed must contain at least one "1" bit, due to how the LFSR works, otherwise it will not be able to cycle through different states. It is important to note that the values provided in the table above may not always achieve 100\% completion, as some differences may occur based on how the oligos are created. 


\newpage
\newpage
\section{Conclusion}

To summarize, in this work, a DNA storage strategy called "DNA Fountain" was presented. It consists of two main stages - encoding and decoding, both of which work together to enable virtually unlimited data retrieval. According to the authors in \cite{erlich2017dna}, the suggested data storage strategy enables "high physical density while approaching the Shannon capacity of DNA storage closer than any previous design".\\ 
In the underlying experiments, 2.15 Mbytes of data were encoded and later decoded, while achieving 1.98 bits/nt coding potential, and only using 7\% redundancy against dropout. This also led to the full recovery of the encoded data, as demonstrated with the implementation part in \ref{subsec:implementation}. In contrast to the authors in \cite{erlich2017dna}, the implementation in \ref{subsec:implementation} required significantly more oligos (up to 3.5 times compared to only 5-10\% more) to be able to fully decode the data. This will introduce more overhead and, most likely, higher prices due to the need of  synthesizing these oligos. The high cost of DNA synthesis is also mentioned in \cite{erlich2017dna}, with data pointing at 3500 dollars / Mbyte. Two general approaches are also mentioned to potentially address the issue of high costs. The first one involves continuous improvements to the DNA synthesis chemistry. The second one aims to achieve cost reduction by exploring "quick-and-dirty" oligo synthesis methods to optimize machine time. \\
Another aspect worth mentioning is that the above suggested strategy does not attempt to correct encoding errors. The supplied extra bits (for example through Reed Solomon Code) only aim to recognize if there were any errors during oligo synthesis or transferring. If such errors are found, the oligos are dropped and ignored. This is a big difference in comparison to other methods that try to correct and identify various errors by using error-correcting algorithms. Moving forward, with the right optimization and investment, DNA storage methods may prove to be cost-efficient and viable long-term solutions.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\linewidth, scale=0.5]{input_small}
    \caption{Small Fraunhofer logo}
    \label{fig:logo_short}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{input_large}
    \caption{Complete Fraunhofer institute logo}
    \label{fig:logo_long}
\end{figure}




\clearpage
\newpage

\bibliography{bib}
\bibliographystyle{unsrt}

\end{document}
