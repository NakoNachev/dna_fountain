\documentclass[12pt]%[english]
{article}

\usepackage{graphicx}
\usepackage{microtype}
\usepackage[nonumberlist, acronym, toc, section]{glossaries}
\usepackage{cite} 
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{caption}


\newglossary[slg]{symbolslist}{syi}{syg}{Symbols}

\makeglossaries

%commands for symbols
\newglossaryentry{symb:Pi}{
name=$\pi$,
description={You know it.},
sort=symbolpi, type=symbolslist
}
\newglossaryentry{symb:Phi}{
name=$\varphi$,
description={At vero eos et accusam et justo duo dolores et ea rebum..},
sort=symbolphi, type=symbolslist
}
\newglossaryentry{symb:Lambda}{
name=$\lambda$,
description={Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy.},
sort=symbollambda, type=symbolslist
}
 
%commands for abbreviations
\newacronym{MS}{MS}{Microsoft}
\newacronym{CD}{CD}{Compact Disc}

% ...

%abbreviations and glossary combined
\newacronym{AD}{AD}{Active Directory\protect\glsadd{glos:AD}}
 
% ... 
 
%glossary commands

\newglossaryentry{glos:AD}{
name=Active Directory,
description={At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet}
}

\newglossaryentry{glos:F}{name=File, description={An arbitrary file}}

% ...

\begin{document}

\begin{titlepage}

\begin{center}

{\Huge %\bfseries
 
{%\textbf{
\textsc{Title}}}
%}
\\[2ex]

{\large %\bfseries 

{Subtitle}}\\[0.5cm]
%\rule{\textwidth}{1pt}


\textbf{
\Large Seminar Datenmanagement\\ 
Prof. Dr. Lena Wiese \\ 
Semester 7
}

\includegraphics[scale=0.4]{logo.jpg} \\ 
\large{\textbf{Institute of Computer Science \\ Goethe-Universit\"at Frankfurt a. M.}}



\begin{Large}
\begin{tabular}{ll}
Author: & \textsc{Nako Nachev} \\
& 6464679\\
& s6327063@stud.uni-frankfurt.de \\
Degree:&  Master Informatik, Semester 7 \\
Module ID: & DM-MS, M-DS-S, DM-BS, B-ATAI-S \\
Date: & \today \\		
\end{tabular}
\end{Large}

\end{center}

\vspace*{\fill}

\large
\noindent{}Work based on: \\
\begin{itemize}
\item Paper 1: \emph{Put Original Author Names here} Put Original Title(s) here
\item Paper 2: \emph{Put Original Author Names here} Put Original Title(s) here
\item ...
\end{itemize}


\end{titlepage}

\newpage\thispagestyle{empty}~ %empty page
\newpage 

\begin{abstract}
\end{abstract}

\newpage

\tableofcontents

\newpage

\listoffigures
\listoftables

\newpage

\section{Introduction}

\subsection{Summary Overview Paper}

The paper in \cite{mercier2010survey} provides a detailed survey about different error correcting codes, their implementation
and associated obstacles with each of the codes. The paper begins by examing the historical significance of error synchronisation problems in
communication systems dating back to Samuel Morse, which remain an important topic even in todays technological systems. Although often treated as
different problems, synchronisation and additive noise tend to have the same effect on communcation channels by reducing their capacity.According to the 
article, designing error-correcting codes to tackle these problems proves to be quite challenging, with many techniques implementation not based on coding or even 
failing to mention synchronisation-correcting codes. \\
 This article continues with a brief introduction on the error-correcting codes and explaining the 
different preliminaries associated with the survey. Such preliminaries include the mathematical notations used, the different types of synchronisation errors and their definitions.
These range from deletion errors, insertion errors, duplication or repetition errors to bitshift errors. Last but not least, obstacles associated with the 
error correcting codes are described. Examples of such are huge bursts of substition errors, splitting long messages of data into blocks of with block boundaries
sometimes being unknown to receiver or codes being unable to keep the same correction capability when encoding several blocks. The authors describe in section III different 
synchronisation-correcting codes, how they are implemented, the historic research done by various authors and specific limitations for the different types. The section starts
with binary algebraic block codes using (0,1) alphabets, nonbinary and perfect codes, codes capable of correcting bursts of synchronisation errors, synchronizable, marker codes, codes for weak
synchronisation errors, convolutional codes, expurgated and codes for random synchronisation channels. \\
The last section of the paper discusses future directions of synchronisation error-correcting codes. Binary block codes are currently unable to detect more than one synchronisation
errors per block. Further challenges arize in estimating the capacity of channels corrupted by synchronisation errors and such where random deletion and/or insertion errors can occur.
According to the authors, none of the codes described in the paper have all the properties required to be used in pratical communication systems. One solution could be concatenating different
coding schemes in order to use the strengths of the various codes for their specific implementation use cases. \\
Deriving from the examples in the article from above, one could divide the error-correcting codes in multiple different categories. One separation used could be based on the number of symbols used in the alphabet - either binary alphabet consisting of (0,1)* symbols or non-binary with 2 or more symbols. Another categorization could follow based on other characteristics - such as block codes vs convolutional codes. Block codes break the data down to fixed-sized blocks and append protective or unique bits for error handling. Convolutional codes on the other side process the data continuously as a stream and do not split the data into blocks. Error correction here is done on the fly. \\
As far as characteristics of the error-correcting codes are concerned, performance, redundancy, error detection and correction seem to be the most important ones. Redundancy finds its application in appending the extra bits for the receiver to make use of and identify errors that may have happended during the transmition. Performance concerns the rate at which algorithms are able to correct the data, where similar to most systems a good balance between correctness achived and computation should be achieved. Error detection and correction seems to be closely related and are the general characteristics the prove how valuable a given error-correcting code is. \\
There are various implementations of error-correcting codes. One such example is by using error-correcting output coding for improving text classification and thus reducing text classification errors by 66 percent.\cite{ghani2000using} Another example is by using error-correcting codes for real time communication in wireless networks. \cite{elaoud1998adaptive} The authors simulate using an adaptive scheme for reducing bandwidth overhead by comparing it to a single code scheme.


\subsection{Important Terms and Definitions}

Upon reading the overview paper, following important terms and definitions could be listed based on the information in \cite{mercier2010survey}:
\begin{itemize}
\item synchronisation error - it is either a delition or an insertion error and excludes substitution errors. For example transmitting 0011 instead of 00011 is a deletion error and transmitting 0011 instead of 011 - insertion error.
\item duplication or repetition error - it is a special insertion that replaces a bit by two copies of the same bit.
\item bitshift error - it a special kind of error that might reverse a given string - for example 01 being transmitted as 10.
\item additive noise - disturbances or interference when transmitting signal over a communication network
\item redundancy - additional information introduced by error-correcting codes to correction synchronisation errors
\item run - a substring of identical symbols of maximum length
\item blocks - used to form chunks of words that are used to divide long messages
\end{itemize}


\newpage

\section{In-Depth Explanation of Error-correcting Code Chosen}

Current storage medias do not have the capacity to meet the demand for the total amount of worldwide data. On top of that, the costs for maintaining and transferring data tend to increase as well, therefore better solutions are needed \cite{dong2020dna}. One such promising solution are the innovate DNA storage systems. They enable us to store digital data into DNA molecules, which are syntetically syntesized and far exceed the lifespan of general storage mediums \cite{dong2020dna}. But transfering and storing data comes with its complications and certain errors could appear, leading to corrupted or incomplete data. Thus, solutions are needed for correcting these errors. The storing strategy provided in \cite{erlich2017dna} makes use of the so called DNA fountains. Fountain codes are typically used for channels with erasures, where files are split into packets and each of these packets is either received without errors or dropped \cite{mackay2005fountain}.  Standard transfer protocols keep transmitting packets until they are successfully received from the other side, where another channel is also required to keep track of the state of transmitted packets or those that require restransmission. Fountain codes on the other side create packets as a result of functions of the whole file and the receiver only has to collect any N packets, with $N \textgreater K$ (original size) so that the whole file can be recovered.In the methaphorical sense, a dna fountain generates droplets representing packets of data und everyone who wants to collect these droplets has to put a bucket under the fountain and wait until the amount collected is larger than original size K of the data \cite{mackay2005fountain}. Due to the nature of the fountain, such DNA fountains can be classified as rateless, since the number of packets generated could be limitless.
The strategy for data storage using DNA fountains proposed in \cite{erlich2017dna} works in two stages. 

\subsection{DNA Fountain encoding}

The first stage is the encoding phase, which consists of three stages - preprocessing, luby transform and screening.

\subsubsection{Preprocessing}  
The first step of the encoding is the preprocessing stage. It begins by packaging the files we want to encode in a tar file and compressing them using stardard lossless algorithms \cite{erlich2017dnasupplementary}.It partitions the file non-overlapping segmets of identical length. A simple example would be splitting the code 001001100101 into segments of length 4, resulting in the following segments:
\begin{itemize}
\item Segment 1: 0010
\item Segment 2: 0110
\item Segment 3: 0101
\end{itemize}

\subsubsection{Luby Transform}
The next step of the enconding is the so-called \textbf{Luby-transform stage}. The first job of the Luby Transform stage is to initialize a pseudonumber number generator (PRNG) with the help of a seed. The seed used by the authors in \cite{erlich2017dnasupplementary} corresponds to the current state of the PRNG, with each cycle for the seed generated with the help of a Linear Feedback Shift Register (LFSR) \cite{erlich2017dnasupplementary}. A LFSR is a shift register whose input bit is a linear function of its previous state \cite{hathwalia2014design}. A LFSR uses a polynomial to generate the so called "taps", which correspond to the coefficients of the polynomial. It works by first shifting the n-bit large string to the right and then XORs together the bits from the previous state corresponding to the "tap" positions. Given the right polynomial, a LFSR can be considered a "maximum-length LFSR", since it is able to produce \(2^{n}-1\) states before returning to the initial state \cite{hathwalia2014design}. In the context of the dna fountain strategy, such LFSR will examine each seed in a given interval without repetition \cite{erlich2017dnasupplementary}. Below is a quick explanation on how the LFSR works: 

\begin{itemize}
\item Suppose we generate a 3-bit seed
\item Set the initial seed: 100
\item Provide a polynomial for the LFSR: \(x^{2} + x + 1\)
\item The above polynomial defines taps on positions (2,1)
\item Shift the seed to the right:  \texttt{\_10}
\item XOR together the bits under the tap positions: \(0 \oplus 0 = 0\)
\item Append the result from the XOR operation to the left, leaving us with \textbf{010} as next state 
\end{itemize}

After generating a seed, the next job of the Luby Transform is to decide how many of the segments to combine together. This is achieved with the help of a special robust soliton distribution, defined as follows \cite{erlich2017dnasupplementary}:   

\[
\rho(d) = 
\begin{cases} 
\frac{1}{K} & \text{if } d = 1, \\
\frac{1}{d(d-1)} & \text{for } d = 2, \dots, K.
\end{cases}
\]

\[
\beta(d) = 
\begin{cases} 
\frac{s}{Kd} & \text{for } d = 1, \dots, \left(\frac{K}{s}\right) - 1, \\
\frac{s \ln(s/\delta)}{K} & \text{for } d = \frac{K}{s}, \\
0 & \text{for } d > \frac{K}{s},
\end{cases}
\]


\[
\mu_{K,c,\delta}(d) = \frac{\rho(d) + \beta(d)}{Z},
\]

with 

\[
s = c \sqrt{\frac{K}{\ln^2\left(\frac{K}{\delta}\right)}} \]

and 
\[
Z = \sum_{d} \rho(d) + \beta(d) = 1
\]


After the distributions for the robust soliton distribution are calculated, N segments are sampled at randomly without replacement (based on the choice from soliton) and  than added bitwise together. The seed generated from the LFSR is attached to the result of the bitwise addition, thus forming the final droplet. One simple example might be:


\begin{itemize}
\item Choose two random segments: 0010 and 0110
\item Apply bitwise addition: \( 0010 \oplus 0110 = 0100 \)
\item Generate seed: 11
\item Attach seed to the result of the bitwise addition to form the droplet: 110100
\end{itemize}


Apart from the seed and payload, extra redundancy can be added to the droplet in order for it to be able to correct various errors that may appear during the syntesizing process. This is where Reed-Solomon codes (R-S) come into place. Reed-Solomon codes are nonbinary cyclic codes with symbols up to m-bit, with m being any positive integer having a value greater than 2. R-S (n,k) codes of m-bit symbols comply to the following rules:
\begin{center}

$0 \textless k \textless n \textless 2^m +2$

\end{center}

with k being the number of symbols encoded and n the total number of symbols in the encoded block \cite{sklar2001reed}. Furthermore, R-S(n,k) codes also include parity symbols:

\begin{center}

$(n,k)=(2^m - 1, 2^m -1 - 2t)$

\end{center}

with $2t = n - k$ being the number of parity symbols \cite{sklar2001reed}. For example a R-S(5,3) 3-bit code  contains 5 code word bytes, of which 3 are data and 2 parity (n-k).Therefore, this code is capable of correcting up to $2t=2   => t = 1$ errors.


\subsubsection{Screening}
The luby transform stage concludes with a screening stage.This stage first converts the achieved droplet to DNA using the following convertion rules: \{00,01,10,11\} to \{A,C,G,T\} \cite{erlich2017dnasupplementary}. After that, the result dna sequence is screened against invalid sequences, such a homopolymer runs or GC content. Long homopolymer runs and or sequences with high GC concentration are undesirable \cite{erlich2017dna} due to being more difficult to syntesize or are more prone to sequencing errors. DNA sequences containing one of the above described properties are rejected and those that pass added to the oligo design file. In \cite{erlich2017dna}, the created droplets were 38 bytes (4 bytes for seed, 32 bytes for data payload and 2 bytes for RS code). Therefore, the values for homopolymer runs were set to \(<= 3 nt \) (nucleotides) and 45-55\% GC content, but these values could be changed based on the specific case. Below is the screening stage described with continuation of the example above:

\begin{itemize}
\item Convert the droplet to a DNA sequence: $110101 \rightarrow TCC$
\item Check against screening rules: TCC does not contain long homopolymer runs or high \% GC content, therefore does not get rejected
\item The oligo file now contains the droplet TCC
\end{itemize}

Since quite a few of the oligos can fail, the authors in \cite{erlich2017dna} recommend aiming for 5-10\% more oligos than input segments. The process of luby transform and screening stage is repeated until that number is reached. 

\newpage
\subsection{DNA Fountain decoding}

The decoding part of the DNA Fountain encoding strategy consists also of 3 stages - preprocessing, droplet recovery and segment interference.

\subsubsection{Preprocessing}

The preprocessing phase of the decoding process begins by filtering out all sequences that do not equal the predefined length \cite{erlich2017dnasupplementary}. Subsequently, idential sequences are dropped and the number of occurences is stored, ensuring that more prevalent sequences will appear first when sorted. As the decoder may not always need to scan all oligos, some of the singletons (sequences that have been observed only a small number of times) may be omitted and the decoder would not try to decode them. Below is a simple example illustrating the process:

\begin{itemize}
\item Assume these are the generated oligos from the encoding process: GAT, TAG, CAG, GTC, GAT, AAG, GAT, GGAT, CAG
\item Step 1: Remove oligos larger than 3nt - GGAT is filtered out
\item Step 2: Calculate occurrences - GAT: 3, TAG: 1, CAG: 2, GTC: 1, AAG: 1
\item Step 3: Sort sequences based on occurrence - GAT:3, CAG: 2, TAG: 1, GTC: 1, AAG: 1
\end{itemize}

\subsubsection{Droplet Recovery}\label{subsec:droplet_recovery}

The second phase of the decoding process is droplet recovery. The generated DNA sequences from the encoding process have to first be translated back into binary using the following scheme, as detailed in  \cite{erlich2017dnasupplementary}: {A,C,G,T}, to {0; 1; 2; 3}. In addition to translating the sequences, the droplet recovery stage must also extract the seed, payload and error-correcting code (if it exists). Unlike other methods, the in \cite{erlich2017dna} described approach does not try to recover sequences with errors. Instead, they are excluded without attempting to correct them. Most of the errors found are mainly due to short insertions and deletions that may result during the oligo synthesis. The Reed Solomon code used in the encoding process is designed to handle only substitution errors, therefore it is not advisable to attempt recovery, as mentioned in \cite{erlich2017dnasupplementary}.


\subsubsection{Segment inference}

The final step in the decoding process is the segment inference. Once the integrity of the message is verified (meaning that all checks from the previous stages have passed), the decoder initializes the PRNG with the extracted seed obtained from \ref{subsec:droplet_recovery}. Using this seed, a list of segment identifiers can be generated as described in \cite{erlich2017dnasupplementary}. These identifiers represent potential positions of the input segments that were combined to form the droplet, as the same seed was used for the PRNG initialization during the encoding process. \\
Next, a message-passing algorithm is initiated to try and infer the segments of each droplet. The algorithm first checks whether the droplet contains segments that have been previously inferred. If such are found, they are XOR-ed from the droplet, and their index is removed from the list of indices. When only one segment remains in the droplet, the segment will be assigned to the droplet data payload. The information regarding the inferred segments is propagated to the remaining droplets, and the entire algorithm is recursively repeated until no more updates can be made. This leads to the number of solved segments slowly increasing until the number of observed oligos reaches the number of segments. At this point, the number of solved segments explodes, as illustrated in the picture below:

 \begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{decoding_oligos}
    \caption{Solved to observed oligos comparison \cite{erlich2017dnasupplementary}}
\end{figure}


\section{Implementation of  Error-correcting Code Chosen}

\subsection{DNA Fountain implementation}

The suggested in \cite{erlich2017dna} dna fountain method was implemented in python by making use of standard libraries and a couple of extra libraries for ready implementations of the LFSR and RS-Codes. The project contains the following files:
\begin{itemize}
\item \texttt{dna\_fountain.py}
\item decoder.py
\item helper.py
\item \texttt{image\_creator.py}
\item models.py
\item soliton.py
\item input.py
\end{itemize}

The starting point of the project is the \texttt{dna\_fountain.py} file. The file begins by initializing the parameters needed for the soliton distribution and the encoding process. The first step involves choosing the input data to use. For the purpose of the seminar, two different inputs were predefined (\texttt{picture\_short} and \texttt{picture\_long}), both of which are saved in the input.py file. Both inputs are Python lists with 0s and 1s, corresponding to the different bits saved from the pictures. Based on the chosen input, different globals and defaults are created. Examples of such are \texttt{segment\_size}, \texttt{initial\_seed}, polynomial and \texttt{segments\_multiplier}. The function continues by first splitting the input list into multiple segments based on the initialized parameters. If the input file is not divisible by the number of bits per segment due to the chosen segment size, the last segment is filled with 0s on the left side to match the size of the other segments. After the list of segments is created, it is passed to the \texttt{robust\_soliton\_distribution} function from the soliton.py file, which returns the distribution of the values based on the total number of segments generated. Soliton.py also contains a \texttt{ideal\_soliton\_distribution} function, which is not used, but remains an option. \\
After all defaults, parameters and values are initialized, the oligo creation process via the \texttt{oligo\_creation} functions begins. It first defines the total number of oligos to be generated and starts iterating until the desired number is reached. The \texttt{luby\_transform} function is responsible for the creation of oligos. It first creates a droplet (with the correspondig seed, data payload and RS-Code (if exists)) and checks it against homopolymer runs or high GC content. If any of the conditions are met, the function with not return anything, meaning the iterator has to try again. Otherwise, the function returns the created oligo, and it is passed to the final list. The core of the \texttt{luby\_transform} function is the \texttt{create\_droplet\_bin} function. It creates the seed by using the LFSR with the predefined parameters (seed, polynomial) and chooses a number N of segments to combine, with the help of the generated distribution from the robust soliton implementation.  The seed is then used to generate N indices for random segments of the list, which are XOR-ed together to form the final payload of the droplet. Along the way, different helper functions and models are used. Typical examples of helper functions include binary arrays to bytes, XOR-ing integers or strings or different utility functions, all of which provided in helper.py file. Models.py contains classes representing different types, which, although not necessary in python due to missing static checking, might improve readability and debugging (if necessary). \\
Once the oligos are created, they are passed, along with other parameters to the decoder.py file. This file starts by preprocessing the oligos. The first two stages of preprocessing, as suggested by \cite{erlich2017dnasupplementary}, involve retaining only sequences of a certain length and collapsing identical sequences and sorting them so that sequences that occur more often are at the front of the list. Since the created oligos are still in string format (due to nucleotides being the letter A,C,G,T), we need to convert the data back to binary. This is achieved via the \texttt{create\_oligo} function, which also extracts the seed, data payload and potential error correcting codes that were appended at the end of the oligo. The described in \cite{erlich2017dna} process does not attempt to make error corrections and for that purpose oligos with errors are dropped and do not fall into the decoding process. Last but not least, the \texttt{recursively\_infer\_segments} function is called, which is the final stage of the decoding process. It iterates through the preprocessed oligos and attempts to infer the segments until no more updates can be made. It finally returns the inferred segments and they are passed back to the \texttt{dna\_fountain.py} file. \\
The final part of the \texttt{dna\_fountain.py} file is the image creation. The \texttt{image\_creator.py} file is called, which contains a function for generating a .PNG picture based on the returned data (list of 0s and 1s). That way we can compare the output picture to the initial picture and compare whether we have successfully decoded the initial file. Since some of the bits may be different, which most of the time is might not be noticable with a human eye, the \texttt{calc\_similarity} function from the helper.py file can compare both input and output lists and return a value between [0,1] indicating how similar the decoded file is to the initial. Value of 1 means the initial file was decoded without any errors or missing information, anything below it would indicate missing or incorrect data.


\subsection{Benchmark Results}

In order to test the decoding power and correctness of the in 3.1 described implementation, a benchmark dataset comprised of two different datasets was used. The first dataset is the leftmost logo part of the Fraunhofer Institute logo \ref{fig:logo_short}, which is afterwards converted to a python list of 0s and 1s and saved into the input.py file. The second dataset in \ref{fig:logo_long} is the complete Fraunhofer logo, which is then converted to a binary list similarly to the transformation of the first dataset. Both of the generated pictures are black and white and do not take into account the original colours of the logo. For testing purposes a third set was also used, comprised of only 16 bits, which enables easier tracking of the whole encoding/decoding process.  \\
White creating output pictures based on the decoded data a 100\% completeness was achieved for both logos. Depending on the chosen logo, different parameters were used for the various stages in the encoding/decoding. Examples of such parameters are the size of the segments in bits, the initial seed, the chosen polynomial for the LFSR and the segments multiplier. Both c and delta values for the soliton distribution remained the same, although they could just as well be made to change accordingly, based on predefined conditions. The table below describes the parameters used for each of the tests: \\


\begin{table}[ht]
    \centering
    \small
    \begin{tabularx}{\textwidth}{|X|X|X|X|}
        \hline
        Dataset & Minimal dataset & Short dataset & Long dataset \\
        \hline
        Segment size & 4 & 32 & 4 \\
        \hline
        Initial seed & 1000 & 1..0 & 1..0 \\
        \hline
        Seed size & 4 bits & 32 bits & 32 bits \\
        \hline
        Polynomial & \(x^{4} + x^{3} + 1 \) & \(x^{32} + x^{30} + x^{26} + x^{25} + 1\) & \(x^{32} + x^{30} + x^{26} + x^{25} + 1\) \\
        \hline
        Segments multiplier & 5 & 3.5 & 3 \\
        \hline
        Soliton c value & 0.2 & 0.2 & 0.2 \\
        \hline
        Soliton delta value & 0.001 & 0.001 & 0.001 \\
        \hline
    \end{tabularx}
    \caption{Benchmark test parameters}
    \label{tab:benchmark-parameters}
\end{table}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{input_small}
    \caption{Small Fraunhofer logo}
    \label{fig:logo_short}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{input_large}
    \caption{Complete Fraunhofer institute logo}
    \label{fig:logo_long}
\end{figure}

\newpage
\section{Conclusion}

\begin{itemize}
\item Summarize the main points and achievements
\item Add your own assessment/criticism on the topic
\end{itemize}



\begin{itemize}
\item You can use abbreviations, like \gls{AD}, \gls{MS} or \gls{CD}.
\item There are also symbols like for example \gls{symb:Pi}, \gls{symb:Phi} and \gls{symb:Lambda}.
\item Last but not least, use glossary entries like \gls{glos:AD} and \gls{glos:F}.
\item Do not forget to cite related work, like \cite{okman2011security} or \cite{borthakur2011apache}.
\end{itemize}
\newpage

%print glossary
\printglossary[style=altlist,title=Glossary]
 
%print abbreviations
\printglossary[type=\acronymtype,style=long]
 
%print symbols
\printglossary[type=symbolslist,style=long]

\newpage

\bibliography{bib}
\bibliographystyle{unsrt}

\end{document}
